{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "96d817b5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "\n",
        "@triton.jit\n",
        "def matrix_multiplication_kernel(\n",
        "    a, b, c, M, N, K, stride_am, stride_an, stride_bn, stride_bk, stride_cm, stride_ck\n",
        "):\n",
        "    row = tl.program_id(0)\n",
        "    col = tl.program_id(1)\n",
        "    acc = tl.zeros((), dtype=tl.float32)\n",
        "    for n in range(N):\n",
        "        a_val = tl.load(a + row * stride_am + n * stride_an)\n",
        "        b_val = tl.load(b + n * stride_bn + col * stride_bk)\n",
        "        acc += a_val * b_val\n",
        "    tl.store(c + row * stride_cm + col * stride_ck, acc)\n",
        "\n",
        "\n",
        "# a, b, c are tensors on the GPU\n",
        "def solve(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor, M: int, N: int, K: int):\n",
        "    stride_am, stride_an = N, 1\n",
        "    stride_bn, stride_bk = K, 1\n",
        "    stride_cm, stride_ck = K, 1\n",
        "\n",
        "    grid = (M, K)\n",
        "    matrix_multiplication_kernel[grid](\n",
        "        a, b, c, M, N, K, stride_am, stride_an, stride_bn, stride_bk, stride_cm, stride_ck\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4398611",
      "metadata": {},
      "source": [
        "Why current kernel is slow:\n",
        "Launches one Triton program per output element (grid=(M, K)), so each program computes only 1 scalar.\n",
        "No tiling/blocking, so global memory reuse is poor (same a/b values reloaded many times).\n",
        "No vectorized loads/stores or tensor-core-friendly structure.\n",
        "No masking for edge tiles; only safe for clean bounds and contiguous assumptions.\n",
        "No autotuning (BLOCK_M/N/K, num_warps, num_stages) for your GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da717ab1",
      "metadata": {},
      "source": [
        "A better kernel should:\n",
        "Use block tiling (e.g. BLOCK_M x BLOCK_N output tile, reduce over BLOCK_K chunks).\n",
        "Keep an accumulator tile in registers (tl.zeros((BLOCK_M, BLOCK_N), tl.float32)).\n",
        "Use masked loads for edge tiles.\n",
        "Use grouped program ordering for better L2 locality.\n",
        "Add @triton.autotune(...) configs per shape regime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8484df15",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Autotune tries multiple launch/block configurations and caches the best one\n",
        "# for each (M, N, K) shape triple.\n",
        "@triton.autotune(\n",
        "    configs=[\n",
        "        # Good default for medium shapes.\n",
        "        triton.Config({\"BLOCK_M\": 64, \"BLOCK_N\": 64, \"BLOCK_K\": 32, \"GROUP_M\": 8}, num_warps=4, num_stages=3),\n",
        "        # Wider along M can help when output has many rows.\n",
        "        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 64, \"BLOCK_K\": 32, \"GROUP_M\": 8}, num_warps=8, num_stages=3),\n",
        "        # Wider along N can help when output has many columns.\n",
        "        triton.Config({\"BLOCK_M\": 64, \"BLOCK_N\": 128, \"BLOCK_K\": 32, \"GROUP_M\": 8}, num_warps=8, num_stages=3),\n",
        "        # Larger 2D tile for bigger matrices.\n",
        "        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 128, \"BLOCK_K\": 32, \"GROUP_M\": 8}, num_warps=8, num_stages=4),\n",
        "    ],\n",
        "    # Re-autotune when these runtime dimensions change.\n",
        "    key=[\"M\", \"N\", \"K\"],\n",
        ")\n",
        "@triton.jit\n",
        "def matrix_multiplication_kernel_tiled(\n",
        "    a_ptr,\n",
        "    b_ptr,\n",
        "    c_ptr,\n",
        "    M,\n",
        "    N,\n",
        "    K,\n",
        "    stride_am,\n",
        "    stride_an,\n",
        "    stride_bn,\n",
        "    stride_bk,\n",
        "    stride_cm,\n",
        "    stride_ck,\n",
        "    BLOCK_M: tl.constexpr,\n",
        "    BLOCK_N: tl.constexpr,\n",
        "    BLOCK_K: tl.constexpr,\n",
        "    GROUP_M: tl.constexpr,\n",
        "):\n",
        "    # Single 1D program-id space. We manually map each pid to a 2D output tile.\n",
        "    pid = tl.program_id(axis=0)\n",
        "\n",
        "    # Number of tiles along output rows (M dimension) and output cols (K dimension).\n",
        "    num_pid_m = tl.cdiv(M, BLOCK_M)\n",
        "    num_pid_n = tl.cdiv(K, BLOCK_N)\n",
        "\n",
        "    # Grouping several row-tiles together improves L2 reuse for B tiles.\n",
        "    num_pid_in_group = GROUP_M * num_pid_n\n",
        "    group_id = pid // num_pid_in_group\n",
        "    first_pid_m = group_id * GROUP_M\n",
        "\n",
        "    # Last group may be smaller than GROUP_M.\n",
        "    group_size_m = tl.minimum(num_pid_m - first_pid_m, GROUP_M)\n",
        "\n",
        "    # Convert 1D pid -> (pid_m, pid_n) tile coordinates.\n",
        "    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n",
        "    pid_n = (pid % num_pid_in_group) // group_size_m\n",
        "\n",
        "    # Row indices for this C tile, column indices for this C tile,\n",
        "    # and reduction indices for one K-chunk (here reduction axis is N).\n",
        "    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
        "    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
        "    offs_k = tl.arange(0, BLOCK_K)\n",
        "\n",
        "    # Register accumulator for one BLOCK_M x BLOCK_N output tile.\n",
        "    # Keep float32 accumulation for better numeric stability.\n",
        "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
        "\n",
        "    # Walk the reduction dimension in BLOCK_K chunks.\n",
        "    for k_start in range(0, N, BLOCK_K):\n",
        "        k_offsets = k_start + offs_k\n",
        "\n",
        "        # Pointer grids for A and B sub-tiles:\n",
        "        # A tile shape = [BLOCK_M, BLOCK_K]\n",
        "        # B tile shape = [BLOCK_K, BLOCK_N]\n",
        "        a_ptrs = a_ptr + offs_m[:, None] * stride_am + k_offsets[None, :] * stride_an\n",
        "        b_ptrs = b_ptr + k_offsets[:, None] * stride_bn + offs_n[None, :] * stride_bk\n",
        "\n",
        "        # Boundary masks so edge tiles are safe when M/N/K are not multiples\n",
        "        # of tile sizes.\n",
        "        a_mask = (offs_m[:, None] < M) & (k_offsets[None, :] < N)\n",
        "        b_mask = (k_offsets[:, None] < N) & (offs_n[None, :] < K)\n",
        "\n",
        "        # Out-of-bounds values are treated as zero so math stays correct.\n",
        "        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n",
        "        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n",
        "\n",
        "        # Matrix multiply-accumulate for this chunk.\n",
        "        acc += tl.dot(a, b)\n",
        "\n",
        "    # Compute output pointers for the C tile.\n",
        "    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_ck\n",
        "    c_mask = (offs_m[:, None] < M) & (offs_n[None, :] < K)\n",
        "\n",
        "    # Store final tile to C, with boundary masking for partial edge tiles.\n",
        "    tl.store(c_ptrs, acc, mask=c_mask)\n",
        "\n",
        "\n",
        "# Faster tiled matmul; keeps your original solve(...) untouched.\n",
        "def solve_tiled(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor, M: int, N: int, K: int):\n",
        "    # Basic input validation to fail fast with useful errors.\n",
        "    assert a.is_cuda and b.is_cuda and c.is_cuda, \"All tensors must be CUDA tensors\"\n",
        "    assert a.is_contiguous() and b.is_contiguous() and c.is_contiguous(), \"Tensors must be contiguous\"\n",
        "    assert a.shape == (M, N), f\"Expected a shape {(M, N)}, got {tuple(a.shape)}\"\n",
        "    assert b.shape == (N, K), f\"Expected b shape {(N, K)}, got {tuple(b.shape)}\"\n",
        "    assert c.shape == (M, K), f\"Expected c shape {(M, K)}, got {tuple(c.shape)}\"\n",
        "\n",
        "    # Number of launched programs = number of output tiles.\n",
        "    grid = lambda meta: (triton.cdiv(M, meta[\"BLOCK_M\"]) * triton.cdiv(K, meta[\"BLOCK_N\"]),)\n",
        "\n",
        "    # Pass raw pointers + runtime sizes + physical tensor strides.\n",
        "    # Strides make this work with row-major contiguous tensors explicitly.\n",
        "    matrix_multiplication_kernel_tiled[grid](\n",
        "        a,\n",
        "        b,\n",
        "        c,\n",
        "        M,\n",
        "        N,\n",
        "        K,\n",
        "        a.stride(0),\n",
        "        a.stride(1),\n",
        "        b.stride(0),\n",
        "        b.stride(1),\n",
        "        c.stride(0),\n",
        "        c.stride(1),\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b193242f",
      "metadata": {},
      "source": [
        "1. The Big Picture: Why do we use \"Blocks\"?\n",
        "\n",
        "Imagine you have two huge walls of sticky notes (Matrix A and Matrix B) and you want to calculate a third wall (Matrix C).\n",
        "If you use your original code (the \"scalar\" approach), you hire one worker for every single sticky note on Wall C. Each worker walks all the way over to Wall A, reads one note, walks to Wall B, reads one note, multiplies them, and writes it down. They do this hundreds of times. This is incredibly slow because walking back and forth to the walls (fetching data from GPU memory) takes forever.\n",
        "\n",
        "Triton's Tiled Approach:\n",
        "Instead of one worker per sticky note, we divide Wall C into large squares (e.g., $64 \\times 64$ blocks). We assign a team of workers (a Triton program) to compute that entire block.\n",
        "\n",
        "The team grabs a chunk of Wall A and a chunk of Wall B, brings it to their desk (ultra-fast GPU registers/SRAM), does a ton of math very quickly, and then goes back for the next chunks. Because they grab big chunks at once, they spend much less time walking and much more time calculating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e75f43a3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2D-program-id version.\n",
        "# Easier to reason about than 1D grouped mapping because launch axes directly\n",
        "# correspond to output-tile row/column coordinates.\n",
        "# A -> MxN\n",
        "# B -> NxK\n",
        "# C -> MxK\n",
        "@triton.autotune(\n",
        "    configs=[\n",
        "        # Balanced default tile for many medium-size shapes.\n",
        "        triton.Config({\"BLOCK_M\": 64, \"BLOCK_N\": 64, \"BLOCK_K\": 32}, num_warps=4, num_stages=3),\n",
        "        # Taller output tile: may help when M is relatively large.\n",
        "        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 64, \"BLOCK_K\": 32}, num_warps=8, num_stages=3),\n",
        "        # Wider output tile: may help when K is relatively large.\n",
        "        triton.Config({\"BLOCK_M\": 64, \"BLOCK_N\": 128, \"BLOCK_K\": 32}, num_warps=8, num_stages=3),\n",
        "        # Large square-ish tile for bigger workloads.\n",
        "        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 128, \"BLOCK_K\": 32}, num_warps=8, num_stages=4),\n",
        "    ],\n",
        "    # Triton selects and caches the best config per runtime shape tuple.\n",
        "    key=[\"M\", \"N\", \"K\"],\n",
        ")\n",
        "@triton.jit\n",
        "def matrix_multiplication_kernel_tiled_2d(\n",
        "    a_ptr,\n",
        "    b_ptr,\n",
        "    c_ptr,\n",
        "    M,\n",
        "    N,\n",
        "    K,\n",
        "    stride_am,\n",
        "    stride_an,\n",
        "    stride_bn,\n",
        "    stride_bk,\n",
        "    stride_cm,\n",
        "    stride_ck,\n",
        "    BLOCK_M: tl.constexpr, # tile height rows of C\n",
        "    BLOCK_N: tl.constexpr, # tile width cols of C\n",
        "    BLOCK_K: tl.constexpr, # How much of K we process per step\n",
        "):\n",
        "    # With a 2D grid:\n",
        "    # - axis 0 chooses output row-tile index\n",
        "    # - axis 1 chooses output col-tile index\n",
        "    \"\"\"\n",
        "    Step 1:\n",
        "    When you run a kernel on a GPU, it launches thousands of identical \"programs\" at the same time.\n",
        "    tl.program_id(0) tells this specific program which Row Block of Matrix C it is responsible for.\n",
        "    tl.program_id(1) tells it which Column Block of Matrix C it is responsible for.\n",
        "\n",
        "    Step 2:Figuring out the exact coordinates\n",
        "    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
        "    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
        "\n",
        "    If BLOCK_M is 64, and I am program row #2 (pid_m=2):\n",
        "    $2 \\times 64 = 128$. So my team is responsible for rows 128 through 191.\n",
        "    offs_m literally creates a list of numbers: [128, 129, 130... 191].\n",
        "\n",
        "    Step 3: The Accumulator \n",
        "    The team creates a blank $64 \\times 64$ grid on their desk to hold the running totals of their multiplication. \n",
        "    They will add to this grid over and over again.\n",
        "\n",
        "    Step 4: Walking across the walls(The For Loop)\n",
        "    To calculate Matrix C, you have to multiply across the rows of Matrix A and down the columns of Matrix B. \n",
        "    The length of this journey is dimension N. \n",
        "\n",
        "    Step 5: Loading the data\n",
        "    a_ptrs = a_ptr + offs_m[:, None] * stride_am + k_offsets[None, :] * stride_an\n",
        "    This looks scary but it just calculates memory addresses. GPU memory is actually a 1-dimensional line of data.\n",
        "    To find the start of row 5, you have to jump 5 * length_of_row spaces. That jump size is called a stride.\n",
        "    This line tells the GPU exactly where to find our $64 \\times 32$ chunk of Matrix A and $32 \\times 64$ chunk of Matrix B.\n",
        "\n",
        "    Step 6: The Actual Math \n",
        "    This is where the magic happens. tl.dot uses the GPU's special AI hardware (Tensor Cores). It instantly multiplies the $64 \\times 32$ chunk of A by the $32 \\times 64$ chunk of B, and adds the result to our notebook (acc).\n",
        "\n",
        "    Step 7: Writing it back to Wall C\n",
        "    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_ck\n",
        "    c_mask = (offs_m[:, None] < M) & (offs_n[None, :] < K)\n",
        "    tl.store(c_ptrs, acc, mask=c_mask)\n",
        "\n",
        "    Once the loop finishes walking across all of N, the team's notebook (acc) has the final answers. They figure out the memory addresses for Matrix C (c_ptrs), make sure they don't write past the edge (c_mask), and save the results back to global GPU memory (tl.store).\n",
        "\n",
        "    \"\"\"\n",
        "    pid_m = tl.program_id(axis=0)  # which tile-row in C\n",
        "    pid_n = tl.program_id(axis=1) # which tile-col in C  (this is actually K-tiles)\n",
        "\n",
        "    # Global indices covered by this program instance.\n",
        "    # offs_m: rows in C tile, offs_n: cols in C tile,\n",
        "    # offs_k: chunk positions along reduction dimension (N).\n",
        "    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M) \n",
        "    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
        "    offs_k = tl.arange(0, BLOCK_K)\n",
        "\n",
        "    # Register tile accumulator.\n",
        "    # Keep float32 accumulation for better numerical stability.\n",
        "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
        "\n",
        "    # Matmul reduction axis is N (A: [M,N], B: [N,K]).\n",
        "    # Iterate over N in BLOCK_K chunks.\n",
        "    for k_start in range(0, N, BLOCK_K):\n",
        "        k_offsets = k_start + offs_k\n",
        "\n",
        "        # Build pointer matrices for the current A/B subtile pair.\n",
        "        # A subtile shape: [BLOCK_M, BLOCK_K]\n",
        "        # B subtile shape: [BLOCK_K, BLOCK_N]\n",
        "        a_ptrs = a_ptr + offs_m[:, None] * stride_am + k_offsets[None, :] * stride_an\n",
        "        b_ptrs = b_ptr + k_offsets[:, None] * stride_bn + offs_n[None, :] * stride_bk\n",
        "\n",
        "        # Edge masks prevent out-of-bounds reads when dimensions are not\n",
        "        # multiples of BLOCK sizes. Masked loads return 0.0.\n",
        "        a_mask = (offs_m[:, None] < M) & (k_offsets[None, :] < N)\n",
        "        b_mask = (k_offsets[:, None] < N) & (offs_n[None, :] < K)\n",
        "\n",
        "        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n",
        "        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n",
        "\n",
        "        # Multiply-accumulate current chunk into output tile accumulator.\n",
        "        acc += tl.dot(a, b)\n",
        "\n",
        "    # Compute output pointers and store final tile with output bounds mask.\n",
        "    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_ck\n",
        "    c_mask = (offs_m[:, None] < M) & (offs_n[None, :] < K)\n",
        "    tl.store(c_ptrs, acc, mask=c_mask)\n",
        "\n",
        "\n",
        "# Python wrapper for 2D tiled kernel.\n",
        "def solve_tiled_2d(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor, M: int, N: int, K: int):\n",
        "    # Validate assumptions used by this tutorial-style kernel.\n",
        "    assert a.is_cuda and b.is_cuda and c.is_cuda, \"All tensors must be CUDA tensors\"\n",
        "    assert a.is_contiguous() and b.is_contiguous() and c.is_contiguous(), \"Tensors must be contiguous\"\n",
        "    assert a.shape == (M, N), f\"Expected a shape {(M, N)}, got {tuple(a.shape)}\"\n",
        "    assert b.shape == (N, K), f\"Expected b shape {(N, K)}, got {tuple(b.shape)}\"\n",
        "    assert c.shape == (M, K), f\"Expected c shape {(M, K)}, got {tuple(c.shape)}\"\n",
        "\n",
        "    # 2D launch grid:\n",
        "    #  - first dimension: number of row tiles\n",
        "    #  - second dimension: number of column tiles\n",
        "    grid = lambda meta: (triton.cdiv(M, meta[\"BLOCK_M\"]), triton.cdiv(K, meta[\"BLOCK_N\"]))\n",
        "\n",
        "    # Pass pointers, sizes, and physical strides so pointer math is correct.\n",
        "    matrix_multiplication_kernel_tiled_2d[grid](\n",
        "        a,\n",
        "        b,\n",
        "        c,\n",
        "        M,\n",
        "        N,\n",
        "        K,\n",
        "        a.stride(0),\n",
        "        a.stride(1),\n",
        "        b.stride(0),\n",
        "        b.stride(1),\n",
        "        c.stride(0),\n",
        "        c.stride(1),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "48f91c7e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All matrix multiplication tests passed.\n"
          ]
        }
      ],
      "source": [
        "def run_matrix_mult_tests():\n",
        "    if not torch.cuda.is_available():\n",
        "        raise RuntimeError(\"CUDA GPU is required to run Triton tests\")\n",
        "\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    # (M, N, K) test cases: square and non-square\n",
        "    test_shapes = [\n",
        "        (1, 1, 1),\n",
        "        (2, 3, 4),\n",
        "        (4, 4, 4),\n",
        "        (7, 5, 3),\n",
        "        (16, 32, 8),\n",
        "        (31, 17, 29),\n",
        "    ]\n",
        "\n",
        "    for m, n, k in test_shapes:\n",
        "        a = torch.randn((m, n), device=\"cuda\", dtype=torch.float32)\n",
        "        b = torch.randn((n, k), device=\"cuda\", dtype=torch.float32)\n",
        "        c = torch.empty((m, k), device=\"cuda\", dtype=torch.float32)\n",
        "\n",
        "        solve(a, b, c, m, n, k)\n",
        "        expected = torch.matmul(a, b)\n",
        "\n",
        "        assert torch.allclose(c, expected, atol=1e-4, rtol=1e-4), (\n",
        "            f\"Mismatch for shape A=({m},{n}), B=({n},{k})\"\n",
        "        )\n",
        "\n",
        "    # Deterministic sanity case\n",
        "    a = torch.tensor([[1.0, 2.0], [3.0, 4.0]], device=\"cuda\")\n",
        "    b = torch.tensor([[5.0, 6.0], [7.0, 8.0]], device=\"cuda\")\n",
        "    c = torch.empty((2, 2), device=\"cuda\")\n",
        "    solve(a, b, c, 2, 2, 2)\n",
        "    expected = torch.tensor([[19.0, 22.0], [43.0, 50.0]], device=\"cuda\")\n",
        "    assert torch.allclose(c, expected, atol=1e-5, rtol=1e-5)\n",
        "\n",
        "    print(\"All matrix multiplication tests passed.\")\n",
        "\n",
        "\n",
        "run_matrix_mult_tests()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef20fc41",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
